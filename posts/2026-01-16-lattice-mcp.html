<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="ie ie6" lang="en"><![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"><![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"><![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html lang="en"><!--<![endif]-->
<head>
    <!-- Website Template designed by www.downloadwebsitetemplates.co.uk -->
    <!-- Modified to fit Cryogen.-->
    <meta charset="UTF-8">
    <a rel="me" href="https://mastodon.social/@yogthos"></a>
    <a rel="me" href="https://mas.to/@yogthos">Mastodon</a>
    <a rel="me" href="https://social.marxist.network/@yogthos">Mastodon</a>
    <title>(iterate think thoughts): Stop Round-Tripping Your Codebase: How to Cut LLM Token Usage by 80% Using Recursive Document Analysis</title>
    <meta name="description" content="">
    <meta name="keywords" content="">
    <meta content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" name="viewport">
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="images/ico/apple-touch-icon-144.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="images/ico/apple-touch-icon-114.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="images/ico/apple-touch-icon-72.png">
    <link rel="apple-touch-icon-precomposed" href="images/ico/apple-touch-icon-57.png">
    <link rel="shortcut icon" href="images/ico/favicon.png">
    <!--[if IE]><![endif]-->
    <link href="/css/site.css" rel="stylesheet" type="text/css" />
    
    <!--
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
    -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.7.0/styles/tomorrow-night-eighties.min.css">
    <!--[if lt IE 9]><script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
</head>
<body>

<div id="left">

    <p id="logo">
        <a title="(iterate think thoughts)" href="/index.html">
            <span class="text"><span class="paren">(</sapn><span class="fn">iterate</span>
                <br>
                <span class="text">&nbsp;&nbsp;think</span> 
                <br>
                <span class="text">&nbsp;&nbsp;thoughts<span class="paren">)</span>
            </span>
        </a>
    </p>

    <div id="menucont" class="bodycontainer clearfix">
        <div class="menutitle">
            <p><span class="fa fa-reorder"></span><strong>Menu</strong></p>
        </div>
        <ul class="menu">
            <li ><a title="Home" href="/index.html">Home</a></li>
            <li ><a title="Archives" href="/archives.html">Archives</a></li>
            
            <li ><a title="Tags" href="/tags.html">Tags</a></li>
            
            
            <li >
                <a href="/pages/about.html">About</a>
            </li>
            
            <li><a title="RSS" href="/feed.xml">RSS</a></li>
        </ul>
    </div>
</div>

<div id="right" class="clearfix">
    
<div id="post">
    <div class="post-header">
    <div id="post-meta" class="row">
        <strong>January 16, 2026</strong>
        
    </div>
    <h1><a class="post-title" href="/posts/2026-01-16-lattice-mcp.html">Stop Round-Tripping Your Codebase: How to Cut LLM Token Usage by 80% Using Recursive Document Analysis</a></h1>
</div>
<div>
    
    <p>When you employ AI agents such as Claude, there’s a significant volume problem for document study. Reading one file of 1000 lines consumes about 10,000 tokens. Token consumption incurs costs and time penalties. Codebases with dozens or hundreds of files, a common case for real world projects, can easily exceed 100,000 tokens in size when the whole thing must be considered. The agent must read and comprehend, and be able to determine the interrelationships among these files. And, particularly, when the task requires multiple passes over the same documents, perhaps one pass to divine the structure and one to mine the details, costs multiply rapidly.</p><p><strong>Matryoshka</strong> is a tool for document analysis that achieves over 80% token savings while enabling interactive and exploratory analysis. The key insight of the tool is to save tokens by caching past analysis results, and reusing them, so you do not have to process the same document lines again. These ideas come from recent research, and retrieval-augmented generation, with a focus on efficiency. We'll see how Matryoshka unifies these ideas into one system that maintains a persistent analytical state. Finally, we'll take a look at some real-world results analyzing the <a href='https://git.sr.ht/~foosoft/anki-connect'>anki-connect</a> codebase.<h2></h2></p><h2 id="the&#95;problem:&#95;context&#95;rot&#95;and&#95;token&#95;costs">The Problem: Context Rot and Token Costs</h2><p>A common task is to analyze a codebase to answers a question such as “What is the API surface of this project?” Such work includes identifying and cataloguing all the entry points exposed by the codebase.</p><p><strong>Traditional approach:</strong></p><ol><li>1. Read all source files into context (~95,000 tokens for a medium project)</li><li>2. The LLM analyzes the entire codebase’s structure and component relationships</li><li>3. For follow-up questions, the full context is round-tripped every turn</li></ol><p>This creates two problems:</p><h3 id="token&#95;costs&#95;compound">Token Costs Compound</h3><p>Every time, the entire context has to go to the API. In a 10-turn conversation about a codebase of 7,000 lines, almost a million tokens might be processed by the system. Most of those tokens are the same document contents being dutifully resent, over and over. The same core code is sent with every new question. This redundant transaction is a massive waste. It forces the model to process the same blocks of text repeatedly, rather than concentrating its capabilities on what’s actually novel.</p><h3 id="context&#95;rot&#95;degrades&#95;quality">Context Rot Degrades Quality</h3><p>As described in the <a href='https://arxiv.org/abs/2505.11409'>Recursive Language Models</a> paper, even the most capable models exhibit a phenomenon called context degradation, in which their performance declines with increasing input length. This deterioration is task-dependent. It’s connected to task complexity. In information-dense contexts, where the correct output requires the synthesis of facts presented in widely dispersed locations in the prompt, this degradation may take an especially precipitous form. Such a steep decline can occur even for relatively modest context lengths, and is understood to reflect a failure of the model to maintain the threads of connection between large numbers of informational fragments long before it reaches its maximum token capacity.</p><p>The authors argue that we should not be inserting prompts into the models, since this clutters their memory and compromises their performance. Instead, documents should be considered as <strong>external environments</strong> with which the LLM can interact by querying, navigating through structured sections, and retrieving specific information on an as-needed basis. This approach treats the document as a separate knowledge base, an arrangement that frees up the model from having to know everything.<h2></h2></p><h2 id="prior&#95;work:&#95;two&#95;key&#95;insights">Prior Work: Two Key Insights</h2><p>Matryoshka builds on two research directions:</p><h3 id="recursive&#95;language&#95;models&#95;(rlm)">Recursive Language Models (RLM)</h3><p>The RLM paper introduces a new methodology that treats documents as external state to which step-by-step queries can be issued, without the necessity of loading them entirely. Symbolic operations, search, filter, aggregate, are actively issued against this state, and only the specific, relevant results are returned, maintaining a small context window while permitting analysis of arbitrarily large documents.</p><p>Key point is that the documents stay outside the model, and only the search results enter the context. This separation of concerns ensures that the model never sees complete files, instead, a search is initiated to retrieve the information.</p><h3 id="barliman:&#95;synthesis&#95;from&#95;examples">Barliman: Synthesis from Examples</h3><p><a href='https://github.com/webyrd/Barliman'>Barliman</a>, a tool developed by William Byrd and Greg Rosenblatt, shows that it is possible to use program synthesis without asking for precise code specifications. Instead, input/output examples are used, and a solver engine is used as a relational programming system in the spirit of <a href='http://minikanren.org/'>miniKanren</a>. Barliman uses such a system to synthesize functions that satisfy the constraints specified. The system interprets the examples as if they were relational rules, and the synthesis engine tries to satisfy them. This approach makes it possible to describe what is desired for concrete test cases.</p><p>The approach is to simply show examples of the kind of behavior one wishes the system to exhibit, letting it derive the implmentation on its own. Thus, the emphasis shifts from writing long and detailed step-by-step recipes for behavior to simply portraying, in a declarative fashion, what the desired goal is.<h2></h2></p><h2 id="matryoshka:&#95;combining&#95;the&#95;insights">Matryoshka: Combining the Insights</h2><p>Matryoshka incorporates these insights into a functioning system for LLM agents. A practical tool is provided that enables agents to decompose challenging tasks into a sequence of smaller and more manageable objectives.</p><h3 id="1.&#95;nucleus:&#95;a&#95;declarative&#95;query&#95;language">1. Nucleus: A Declarative Query Language</h3><p>Instead of issuing commands, the LLM describes <strong>what</strong> it wants, using <a href='https://github.com/michaelwhitford/nucleus'>Nucleus</a>, a simple S-expression query language. This changes the focus from describing each step to specifying the desired outcome.</p><pre><code class="clojure">&#40;grep &quot;class &quot;&#41;           ; Find all class definitions
&#40;count RESULTS&#41;           ; Count them
&#40;map RESULTS &#40;lambda x    ; Extract class names
  &#40;match x &quot;class &#40;\\w+&#41;&quot; 1&#41;&#41;&#41;
</code></pre><p>We observe that the declarative interface retains its robustness even when the LLM employs different vocabulary or sentence structures. This robustness originates from the system’s commitment to elucidating the underlying intent of a request, independent of superficial linguistic variations.</p><h3 id="2.&#95;pointer-based&#95;state">2. Pointer-Based State</h3><p>The key new insight is that we can separate the results from the context. Results are now stored in the REPL state, rather than in the context.</p><p>When Claude runs <code>&#40;grep &quot;def &quot;&#41;</code> and gets 150 matches:</p><ul><li><strong>Traditional tools</strong>: All 150 lines are fed into context, and round-tripped every turn</li><li><strong>Matryoshka</strong>: Binds matches to <code>RESULTS</code> in the REPL, returning only "Found 150 results"</li></ul><p>The variable <code>RESULTS</code> is bound to the actual value in the REPL. This binding acts as a pointer, revealing the location of the data within the server's memory. Subsequent operations, queries, for example, or updates, use this reference to access the data. But the data itself never actually enters the conversation:</p><pre><code>Turn 1: &#40;grep &quot;def &quot;&#41;         → Server stores 150 matches as RESULTS
                              → Context gets: &quot;Found 150 results&quot;

Turn 2: &#40;count RESULTS&#41;       → Server counts its local RESULTS
                              → Context gets: &quot;150&quot;

Turn 3: &#40;filter RESULTS ...&#41;  → Server filters locally
                              → Context gets: &quot;Filtered to 42 results&quot;
</code></pre><p>The LLM never sees the 150 function definitions, just the aggregated answers from these functions.</p><h3 id="3.&#95;synthesis&#95;from&#95;examples">3. Synthesis from Examples</h3><p>When queries need custom parsing, Matryoshka synthesizes functions from examples:</p><pre><code class="clojure">&#40;synthesize&#95;extractor
  &quot;$1,250.00&quot; 1250.00
  &quot;€500&quot; 500
  &quot;$89.99&quot; 89.99&#41;
</code></pre><p>The synthesizer learns the pattern directly from examples, obtaining numerical values straight from the currency strings and entirely circumventing the need to construct manual regex.<h2></h2></p><h2 id="the&#95;lifecycle">The Lifecycle</h2><p>A typical Matryoshka session:</p><h3 id="1.&#95;load&#95;document">1. Load Document</h3><pre><code class="clojure">&#40;load &quot;./plugin/&#95;&#95;init&#95;&#95;.py&quot;&#41;
→ &quot;Loaded: 2,244 lines, 71.5 KB&quot;
</code></pre><p>The document is parsed and stored server-side. Only metadata enters the context.</p><h3 id="2.&#95;query&#95;incrementally">2. Query Incrementally</h3><pre><code class="clojure">&#40;grep &quot;@util.api&quot;&#41;
→ &quot;Found 122 results, bound to RESULTS&quot;
   &#91;402&#93; @util.api&#40;&#41;
   &#91;407&#93; @util.api&#40;&#41;
   ... &#40;showing first 20&#41;
</code></pre><p>Each query returns a preview plus the count. Full data stays on server.</p><h3 id="3.&#95;chain&#95;operations">3. Chain Operations</h3><pre><code class="clojure">&#40;count RESULTS&#41;           → 122
&#40;filter RESULTS ...&#41;      → &quot;Filtered to 45 results&quot;
&#40;map RESULTS ...&#41;         → Transforms bound to RESULTS
</code></pre><p>Operations chain through the <code>RESULTS</code> binding. Each step refines without re-querying.</p><h3 id="4.&#95;close&#95;session">4. Close Session</h3><pre><code class="clojure">&#40;close&#41;
→ &quot;Session closed, memory freed&quot;
</code></pre><p>Sessions auto-expire after 10 minutes of inactivity.<h2></h2></p><h2 id="how&#95;agents&#95;discover&#95;and&#95;use&#95;matryoshka">How Agents Discover and Use Matryoshka</h2><p>Matryoshka integrates with LLM agents via the <a href='https://modelcontextprotocol.io/'>Model Context Protocol (MCP)</a>.</p><h3 id="tool&#95;discovery">Tool Discovery</h3><p>When Claude Code starts, it launches Matryoshka as an MCP server and receives a tool manifest:</p><pre><code class="json">{
  &quot;tools&quot;: &#91;
    {
      &quot;name&quot;: &quot;lattice&#95;load&quot;,
      &quot;description&quot;: &quot;Load a document for analysis...&quot;
    },
    {
      &quot;name&quot;: &quot;lattice&#95;query&quot;,
      &quot;description&quot;: &quot;Execute a Nucleus query...&quot;
    },
    {
      &quot;name&quot;: &quot;lattice&#95;help&quot;,
      &quot;description&quot;: &quot;Get Nucleus command reference...&quot;
    }
  &#93;
}
</code></pre><p>Claude sees the available tools and their descriptions. When a user asks to analyze a file, Claude decides which tools to use based on the task.</p><h3 id="guided&#95;discovery">Guided Discovery</h3><p>The <code>lattice&#95;help</code> tool returns a command reference, teaching the LLM the query language on-demand:</p><pre><code class="clojure">; Search commands
&#40;grep &quot;pattern&quot;&#41;              ; Regex search
&#40;fuzzy&#95;search &quot;query&quot; 10&#41;     ; Fuzzy match, top N
&#40;lines 10 20&#41;                 ; Get line range

; Aggregation
&#40;count RESULTS&#41;               ; Count items
&#40;sum RESULTS&#41;                 ; Sum numeric values

; Transformation
&#40;map RESULTS fn&#41;              ; Transform each item
&#40;filter RESULTS pred&#41;         ; Keep matching items
</code></pre><p>The agent learns capabilities incrementally rather than needing upfront training.</p><h3 id="session&#95;flow">Session Flow</h3><pre><code>User: &quot;How many API endpoints does anki-connect have?&quot;

Claude: &#91;Calls lattice&#95;load&#40;&quot;plugin/&#95;&#95;init&#95;&#95;.py&quot;&#41;&#93;
        → &quot;Loaded: 2,244 lines&quot;

Claude: &#91;Calls lattice&#95;query&#40;'&#40;grep &quot;@util.api&quot;&#41;'&#41;&#93;
        → &quot;Found 122 results&quot;

Claude: &#91;Calls lattice&#95;query&#40;'&#40;count RESULTS&#41;'&#41;&#93;
        → &quot;122&quot;

Claude: &quot;The anki-connect plugin exposes 122 API endpoints,
         decorated with @util.api&#40;&#41;.&quot;
</code></pre><p>Each tool invocation maintains its own state within the conversation. So, for example, when a document is loaded, that content is retained in memory. Similarly, the results of any query that is executed are saved and available for later use.<h2></h2></p><h2 id="real-world&#95;example:&#95;analyzing&#95;anki-connect">Real-World Example: Analyzing anki-connect</h2><p>Let's walk through a complete analysis of the <a href='https://git.sr.ht/~foosoft/anki-connect'>anki-connect</a> Anki plugin. Here we have a real-world codebase with 7,770 lines across 17 files.</p><h3 id="the&#95;task">The Task</h3><p>"Analyze the anki-connect codebase: find all classes, count API endpoints, extract configuration defaults, and document the architecture."</p><h3 id="the&#95;workflow">The Workflow</h3><p>The agent uses Matryoshka's <a href='https://github.com/yogthos/Matryoshka/blob/4a2143851eed8245d7a314694a2ba9eb6ab80466/src/lattice-mcp-server.ts#L97'>prompt hints</a> to accomplish the following workflow:</p><ol><li>1. <strong>Discover files</strong> with Glob</li><li>2. <strong>Read small files directly</strong> (<300 lines)</li><li>3. <strong>Use Matryoshka for large files</strong> (>500 lines)</li><li>4. <strong>Aggregate across all files</strong><h2></h2></li></ol><h3 id="step&#95;1:&#95;file&#95;discovery">Step 1: File Discovery</h3><pre><code>Glob &#42;&#42;/&#42;.py → 15 Python files
Glob &#42;&#42;/&#42;.md → 2 markdown files

File sizes:
  plugin/&#95;&#95;init&#95;&#95;.py    2,244 lines  → Matryoshka
  plugin/edit.py          458 lines  → Read directly
  plugin/web.py           301 lines  → Read directly
  plugin/util.py          107 lines  → Read directly
  README.md             4,660 lines  → Matryoshka
  tests/&#42;.py           11 files      → Skip &#40;tests&#41;
</code></pre><h2></h2><h3 id="step&#95;2:&#95;read&#95;small&#95;files">Step 2: Read Small Files</h3><p>Reading <code>util.py</code> (107 lines) reveals configuration defaults:</p><pre><code class="python">DEFAULT&#95;CONFIG = {
    'apiKey': None,
    'apiLogPath': None,
    'apiPollInterval': 25,
    'apiVersion': 6,
    'webBacklog': 5,
    'webBindAddress': '127.0.0.1',
    'webBindPort': 8765,
    'webCorsOrigin': None,
    'webCorsOriginList': &#91;'http://localhost'&#93;,
    'ignoreOriginList': &#91;&#93;,
    'webTimeout': 10000,
}
</code></pre><p>Reading <code>web.py</code> (301 lines) reveals the server architecture:</p><ul><li>- Classes: <code>WebRequest</code>, <code>WebClient</code>, <code>WebServer</code></li><li>- JSON-RPC style API with jsonschema validation</li><li>- CORS support with configurable origins<h2></h2></li></ul><h3 id="step&#95;3:&#95;query&#95;large&#95;files&#95;with&#95;matryoshka">Step 3: Query Large Files with Matryoshka</h3><pre><code class="clojure">; Load the main plugin file
&#40;load &quot;plugin/&#95;&#95;init&#95;&#95;.py&quot;&#41;
→ &quot;Loaded: 2,244 lines, 71.5 KB&quot;

; Find all classes
&#40;grep &quot;&#94;class &quot;&#41;
→ &quot;Found 1 result: &#91;65&#93; class AnkiConnect:&quot;

; Count methods
&#40;grep &quot;def \\w+\\&#40;self&quot;&#41;
→ &quot;Found 148 results&quot;

; Count API endpoints
&#40;grep &quot;@util.api&quot;&#41;
→ &quot;Found 122 results&quot;

; Load README for documentation
&#40;load &quot;README.md&quot;&#41;
→ &quot;Loaded: 4,660 lines, 107.2 KB&quot;

; Find documented action categories
&#40;grep &quot;&#94;### &quot;&#41;
→ &quot;Found 13 sections&quot;
   &#91;176&#93; ### Card Actions
   &#91;784&#93; ### Deck Actions
   &#91;1231&#93; ### Graphical Actions
   ...
</code></pre><h3 id="complete&#95;findings">Complete Findings</h3><p><table> <thead> <tr> <th>Metric</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Total files</td> <td>17 (15 .py + 2 .md)</td> </tr> <tr> <td>Total lines</td> <td>7,770</td> </tr> <tr> <td>Classes</td> <td>8 (1 main + 3 web + 4 edit)</td> </tr> <tr> <td>Instance methods</td> <td>148</td> </tr> <tr> <td>API endpoints</td> <td>122</td> </tr> <tr> <td>Config settings</td> <td>11</td> </tr> <tr> <td>Imports</td> <td>48</td> </tr> <tr> <td>Documentation sections</td> <td>8 categories, 120 endpoints</td> </tr> </tbody> </table></p><h3 id="token&#95;usage&#95;comparison">Token Usage Comparison</h3><p><table> <thead> <tr> <th>Approach</th> <th>Lines Processed</th> <th>Tokens Used</th> <th>Coverage</th> </tr> </thead> <tbody> <tr> <td>Read everything</td> <td>7,770</td> <td>~95,000</td> <td>100%</td> </tr> <tr> <td>Matryoshka only</td> <td>6,904</td> <td>~6,500</td> <td>65%</td> </tr> <tr> <td><strong>Hybrid</strong></td> <td>7,770</td> <td><strong>~17,000</strong></td> <td><strong>100%</strong></td> </tr> </tbody> </table></p><p>The hybrid method achieves a <strong>82% savings</strong> in tokens while retaining 100% of the original coverage. This approach combines two different strategies, one for compressing redundant information and one for preserving unique insights.</p><p>The pure Matryoshka approach ends up missing details from small files (configuration defaults, web server classes), because Claude only uses the tool to query large ones. The hybrid workflow does direct, full-content reads on small files, while leveraging Matryoshka to analyze bigger files, in a kind of divide-and-conquer strategy. All that's needed is to provide the agent an explicit hint on the strategy to use.</p><h3 id="why&#95;hybrid&#95;works">Why Hybrid Works</h3><p>Small files (<300 lines) contain critical details:</p><ul><li>- <code>util.py</code>: All configuration defaults, the API decorator implementation</li><li>- <code>web.py</code>: Server architecture, CORS handling, request schema</li></ul><p>These fit comfortably in context, and there's no need to do anything different. Matryoshka adds value for:</p><ul><li>- <code>&#95;&#95;init&#95;&#95;.py</code> (2,244 lines): Query specific patterns without loading everything</li><li>- <code>README.md</code> (4,660 lines): Search documentation sections on demand<h2></h2></li></ul><h2 id="architecture">Architecture</h2><pre><code>┌─────────────────────────────────────────────────────────┐
│                     Adapters                            │
│  ┌──────────┐  ┌──────────┐  ┌───────────────────────┐  │
│  │   Pipe   │  │   HTTP   │  │   MCP Server          │  │
│  └────┬─────┘  └────┬─────┘  └───────────┬───────────┘  │
│       │             │                    │              │
│       └─────────────┴────────────────────┘               │
│                          │                              │
│                ┌─────────┴─────────┐                    │
│                │   LatticeTool     │                    │
│                │   &#40;Stateful&#41;      │                    │
│                │   • Document      │                    │
│                │   • Bindings      │                    │
│                │   • Session       │                    │
│                └─────────┬─────────┘                    │
│                          │                              │
│                ┌─────────┴─────────┐                    │
│                │  NucleusEngine    │                    │
│                │  • Parser         │                    │
│                │  • Type Checker   │                    │
│                │  • Evaluator      │                    │
│                └─────────┬─────────┘                    │
│                          │                              │
│                ┌─────────┴─────────┐                    │
│                │    Synthesis      │                    │
│                │  • Regex          │                    │
│                │  • Extractors     │                    │
│                │  • miniKanren     │                    │
│                └───────────────────┘                    │
└─────────────────────────────────────────────────────────┘
</code></pre><h2></h2><h2 id="getting&#95;started">Getting Started</h2><p>Install from npm:</p><pre><code class="bash">npm install matryoshka-rlm
</code></pre><h3 id="as&#95;mcp&#95;server&#95;(claude&#95;code&#95;/&#95;claude&#95;desktop)">As MCP Server (Claude Code / Claude Desktop)</h3><p>Add to your Claude configuration:</p><pre><code class="json">{
  &quot;mcpServers&quot;: {
    &quot;lattice&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: &#91;&quot;lattice-mcp&quot;&#93;
    }
  }
}
</code></pre><h3 id="programmatic&#95;use">Programmatic Use</h3><pre><code class="typescript">import { NucleusEngine } from &quot;matryoshka-rlm&quot;;

const engine = new NucleusEngine&#40;&#41;;
await engine.loadFile&#40;&quot;./document.txt&quot;&#41;;

const result = engine.execute&#40;'&#40;grep &quot;pattern&quot;&#41;'&#41;;
console.log&#40;result.value&#41;; // Array of matches
</code></pre><h3 id="interactive&#95;repl">Interactive REPL</h3><pre><code class="bash">npx lattice-repl
lattice&gt; :load ./data.txt
lattice&gt; &#40;grep &quot;ERROR&quot;&#41;
lattice&gt; &#40;count RESULTS&#41;
</code></pre><h2></h2><h2 id="conclusion">Conclusion</h2><p>Matryoshka embodies the principle, emerging from RLM research, that documents are to be treated as external environments rather than as contexts to be parsed. This principle alters the fundamental character of the model’s engagement, no longer a passive reader but an active agent, navigating through and interrogating a document to extract specific information, somewhat as a programmer would browse through code. Combined with Barliman-style synthesis, in which a solution is built up in a series of small, well-defined steps, and pointer-based state management, it achieves:</p><ul><li>- <strong>82% token savings</strong> on real-world codebase analysis</li><li>- <strong>100% coverage</strong> when combined with direct reads for small files</li><li>- <strong>Incremental exploration</strong> where each query builds on previous results</li><li>- <strong>No context rot</strong> because documents stay outside the model</li></ul><p>We observe that variable bindings such as <code>RESULTS</code> refer to REPL state rather than holding data directly in model context. As we formulate and submit queries, what is sent to the server are mere pointers, placeholders indicating where the actual computation should occur. It is the server that executes the substantive computational tasks, returning only the distilled results.</p><p>The tool is open source: <a href='https://github.com/yogthos/Matryoshka'>https://github.com/yogthos/Matryoshka</a></p>
</div>

<div id="post-tags">
    <br/> 
    <b>Tags: </b>
    
    <a href="/llm.html">llm</a>
    
    <a href="/typescript.html">typescript</a>
    
    <a href="/programming.html">programming</a>
    
    <a href="/miniKanren.html">miniKanren</a>
    
    <a href="/lisp.html">lisp</a>
    
</div>

<br/>

    <div id="prev-next">
        
        <a class="button" href="/posts/2026-02-25-ai-at-scale.html">&laquo; Managing Complexity with Mycelium</a>
        
        
        <a class="right button" href="/posts/2026-01-12-recursive-language-model.html">Grounding LLMs with Recursive Code Execution &raquo;</a>
        
    </div>

    


</div>

<hr/>
<div id="footercont" class="clearfix">Copyright &copy; 2026 Dmitri Sotnikov
    <p>Powered by <a href="http://cryogenweb.org">Cryogen</a></p>

</div>
</div>
<script src="//code.jquery.com/jquery-1.11.0.min.js"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script src="/js/scripts.js" type="text/javascript"></script>


</body>
</html>
